<!DOCTYPE HTML>
<html lang="en">

<head>
  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-113053854-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-113053854-1');
  </script>

  <title>Anand Bhattad</title>

  <meta name="author" content="Anand Bhattad">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/uiuc_square.jpg">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:60%;vertical-align:top">
                  <p style="text-align:left;margin:0">
                    <name>Anand Bhattad</name>
                  <p></p>
                  Incoming Research Assistant Professor
                  <br>
                  <a href="https://www.ttic.edu/" target="_blank">Toyota Technological Institute at Chicago</a>
                  <br>
                  <br>
                  Current: PhD Student
                  <br>
                  Advisor: <a href="http://luthuli.cs.uiuc.edu/~daf/" target="_blank">David A. Forsyth</a>
                  <br>
                  <a href="https://cs.illinois.edu/" target="_blank">Computer Science Department</a>
                  <br>
                  <a href="https://illinois.edu/" target="_blank">University of Illinois Urbana Champaign</a>

                  <p></p>
                  <a href="mailto:anandbhattad92@gmail.com" target="_blank">Email</a> &nbsp;/&nbsp;
                  <a href="./files/bhattad_cv_2023.pdf" target="_blank">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=XUsauXIAAAAJ&hl=en">Google Scholar</a>
                  &nbsp;/&nbsp;
                  <a href="https://twitter.com/anand_bhattad" target="_blank">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/anandbhattad" target="_blank">Github</a>
                  <p></p>
                  <img style="width: 60%;padding: 10px 0px;" src="./images/uiuc_horizontal.png">
                  </p>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="./images/anand_bhattad_2022.jpg" target="_blank"><img style="width:90%;max-width:90%" alt="profile photo"
                      src="images/anand_bhattad_2022.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
              <tr>
                <td colspan="2" style="
                border-top: 1px solid black;
                border-bottom: 1px solid black;">
                  <ul class="topbar">
                    <li><a href="#affiliations">Affiliations</a></li>
                    <li><a href="#research">Research</a> </li>
                    <li><a href="#pubs">Publications</a></li>
                    <li><a href="#teaching">Teaching</a></li>
                  </ul>
                </td>
              </tr>
              <tr style="padding:0px">
                <td colspan="2" style="padding: 10px 0 0;width:100%;vertical-align:middle">
                  <p>
                    Excited to join <a href="https://www.ttic.edu/" target="_blank">Toyota Technological Institute at Chicago (TTIC)</a> as a Research Assistant Professor in Fall 2023! Currently, I'm a PhD candidate in the <a href="http://vision.cs.illinois.edu/vision_website/" target="_blank">Computer Vision Group</a> at <a href="https://illinois.edu/" target="_blank">UIUC</a>, mentored by <a href="http://luthuli.cs.uiuc.edu/~daf/" target="_blank">David Forsyth</a>. I also closely collaborate with <a href="https://dhoiem.cs.illinois.edu/">Derek Hoiem</a>, <a href="https://shenlong.web.illinois.edu/">Shenlong Wang</a>, and <a href="https://yxw.web.illinois.edu/">Yuxiong Wang</a>. My passion lies in computer vision and computational photography.
                  </p>
                  <p>
                    My journey started at UIUC, where I earned an MS in Computer Science and discovered my love for computer vision. In previous life, I worked on <a href="./images/damage_assessment_self_powered_sensor.jpeg" target="_blank">Damage Assessment of Civil Infrastructure Systems</a> and completed an MS from UIUC and a BTech from NITK Surathkal in India, both in Civil Engineering.
                  </p>
                  <p>
                    Throughout my PhD, I've had the privilege of collaborating with remarkable researchers and teams:
                    <br>
                    <ul>
                      <li><a href="https://anikem.github.io/" target="_blank">Ani Kembhavi</a> at Allen Institute of AI in 2023</li>
                      <li><a href="http://www.stephanrichter.org/" target="_blank">Stephan Richter</a> at Intel in 2021 (within <a href="http://vladlen.info/" target="_blank">Vladlen Koltun</a>'s Intelligent Systems Lab)</li>
                      <li><a href="https://scholar.google.com/citations?user=pvu770UAAAAJ&hl=en" target="_blank">Aysegul Dundar</a>, <a href="https://liuguilin1225.github.io/">Guilin Liu</a>, <a href="https://scholar.google.com/citations?user=Wel9l1wAAAAJ&hl=en" target="_blank">Andrew Tao</a>, and <a href="https://ctnzr.io/" target="_blank">Bryan Catanzaro</a> at NVIDIA's Applied Deep Learning Research team in 2020</li>
                      <li><a href="https://abhishekkar.info/" target="_blank">Abhishek Kar</a> and <a href="https://scholar.google.fr/citations?user=yZMAlU4AAAAJ&hl=en" target="_blank">Rodrigo Ortiz Cayón</a> in 2019 at Fyusion Inc.</li>
                    </ul>
                  </p>
                    <heading><strong> Get in touch</strong></heading>
                  </p>
                  <p>
                             <!-- I am interested in an academic career and currently seeking a postdoc position starting Fall 2023. 
           View my research statement at <a
                      href="./files/bhattad_research_statement.pdf" target="_blank">here</a>. 
                      Please email me if you think I would be a good fit for your team.  -->
                  </p>
                  <p>
                    I organize <a href="https://vision.cs.illinois.edu/vision_website/#five" target="_blank">computer vision speaker
                      series</a> at UIUC. If you are interested in giving a talk at UIUC, please email me.

                  </p>

                  <heading><strong> Recent and Upcoming Talks</strong></heading>
                  <ul>
                    <li> Stanford University, Jun 2023 </li>
                    <li> University of Tübingen, Autonomous Vision Group, May 2023 </li>
                    <li> UC Berkeley: Vision Seminar, Apr 2023 </li>
                    <li> NVIDIA Research, Apr 2023 </li>
                    <li> MIT: Vision and Graphics Seminar, Apr 2023 </li>
                    <li> CMU: VASC Seminar, Mar 2023 </li>
                    <li> UW: Vision Seminar, Mar 2023 </li>
                    <li> UMD: Vision Seminar, Mar 2023 </li>
                    <li> UCSD: Pixel Cafe Seminar, Feb 2023 </li>
                    <li> TTIC: Research Talk, Feb 2023 </li>
                    <!-- <li> CMU: MISC Reading Group, August 16th, 2021 </li>
                    <li> CVPRW: Vision Seminar, June 03rd, 2020 </li>
                    <li> UIUC: CSL Student Conference, February 03rd, 2023 </li>
                    <li> CVPRW: Vision with Biased or Scarce Data, June 03rd, 2023 </li>  -->

                  </ul>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <p>
                <heading id="affiliations"> Affiliations</heading>
              </p>
              <br>
              <tr align="center" style="vertical-align: middle" background-color:transparent>
                <td align="center" style="vertical-align: middle">
                  <a href="http://www.nitk.ac.in/" target="_blank">
                    <img src="./images/nitk.png" width="60" height="60"></a>
                </td>
                <td align="center" style="vertical-align: middle">
                  <a href="https://www.msu.edu/" target="_blank">
                    <img src="./images/msu.png" width="60" height="60"></a>
                </td>
                <td align="center" style="vertical-align: middle">
                  <a href="https://cs.illinois.edu/" target="_blank">
                    <img src="./images/uiuc_square.jpg" width="60" height="60"></a>
                </td>
                <!-- <br> 
        <tr align="center" style="vertical-align: middle" background-color:transparent> -->
                <td align="center" style="vertical-align: middle">
                  <a href="https://new.siemens.com/global/en/company/innovation/corporate-technology.html"
                    target="_blank">
                    <img src="./images/siemens_square.jpg" width="60" height="60"></a>
                </td>
                <td align="center" style="vertical-align: middle">
                  <a href="https://fyusion.com/" target="_blank">
                    <img src="./images/fyusion_logo.png" width="60" height="60"></a>
                </td>
                <td align="center" style="vertical-align: middle">
                  <a href="https://nvidia.com/" target="_blank">
                    <img src="./images/nvidia_ai.jpg" width="60" height="60"></a>
                </td>
                <td align="center" style="vertical-align: middle">
                  <a href="http://vladlen.info/lab/" target="_blank">
                    <img src="./images/intel_square.png" width="60" height="60"></a>
                </td>
                <td align="center" style="vertical-align: middle">
                  <a href="https://prior.allenai.org/" target="_blank">
                    <img src="./images/ai2_logo.png" width="60" height="60"></a>
                </td>
                <!-- </tr> -->
              </tr>
              <tr>
                <td align="center" style="vertical-align: middle">
                  <font size="3">NITK Surathkal<br>2011-2015</font>
                </td>
                <td align="center" style="vertical-align: middle">
                  <font size="3">MSU<br>Summer 2014</font>
                </td>
                <td align="center" style="vertical-align: middle">
                  <font size="3">UIUC<br>2015-Present</font>
                </td>
                <td align="center" style="vertical-align: middle">
                  <font size="3">Siemens<br>Summer 2017</font>
                </td>
                <td align="center" style="vertical-align: middle">
                  <font size="3">Fyusion Inc<br>Summer 2019</font>
                </td>
                <td align="center" style="vertical-align: middle">
                  <font size="3">NVIDIA<br>Summer & Fall 2020</font>
                </td>
                <td align="center" style="vertical-align: middle">
                  <font size="3">Intel<br>Spring & Summer 2021</font>
                </td>
                <td align="center" style="vertical-align: middle">
                  <font size="3">AI2<br>Spring 2023</font>
                </td>
              </tr>
            </tbody>
          </table>
          <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>   

      <tr >

      </tr>
</tbody></table> -->
          <br>
          <p>
            <heading id="research"> Research</heading>
          </p>
          <p> 
            I am passionate about data-driven approaches for developing methods for <span style="color:blue">understanding</span>, <span style="color:#028502">modeling</span>, and <span style="color:#c48615">recreating</span> the visual world around us. I believe that generative models have the potential to revolutionize the way we interact with the visual world.
          </p>
          <p> During my PhD, I primarily focused on image relighting, object insertion, and novel view synthesis of scenes by using generative models. 
            I aimed to create algorithms that allow users to perform these tasks easily, controllably, and convincingly. For example, my method for relighting images allows users to change the lighting of an image without having to manually select the light sources by manipulating latent code in StyleGAN.</p>
          <p>
            In the next few years, I plan to explore generative models for advanced visual perception and neural rendering. In particular, I am interested in:            
            
            <details>
              <summary> <span style="color:blue">Emergent Properties in Generative Models</span> </summary>
              Generative models are capable of creating realistic and complex images, but we don't yet fully understand how they do it. In particular, we don't understand how generative models are able to learn the underlying statistical structure of the data they are trained on. I believe that understanding these emergent properties is essential for developing more powerful and effective generative models. </p>
            </details>          
            <details>
              <summary> <span style="color:#028502">Generative Models as the Backbone of Visual Perception</span> </summary>
              State-of-the-art computer vision relies on using generative models for creating realistic images and data, and discriminative models for classifying and understanding them. I envision a future in which generative models serve as the foundation for modern computer vision systems. Moving beyond the current divide-and-conquer strategy for generative and discriminative tasks, my goal is to establish a single framework capable of effectively performing both generative and discriminative tasks. </p>
            </details>
            <details>
              <summary> <span style="color:#c48615">Generative Models as Next-Generation Neural Rendering Engines</span> </summary>
              Traditional rendering engines are based on physically-based models that simulate the way light interacts with objects in the real world. However, these models can be computationally expensive to run, and they may not be able to capture the full complexity of real-world scenes. Generative models offer a more efficient and flexible alternative. My research will explore the creation of realistic and interactive graphics by using generative models. </p>
            </details> 
          </p>
          <p> </p>
          <br>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <p>
                <heading id="pubs">Publications</heading>
              </p>
              <br>
            <!-- StyleGAN knows Normal, Depth, Albedo and more -->
              <tr onmouseout="stylegan_stop()" onmouseover="stylegan_start()">
                <td class="paper-img">
                  <div class="one">
                    <div class="two" id='stylegan_knows'>
                      <img src='./images/stylegan_knows.gif' width="160">
                    </div>
                    <img src='./images/stylegan_knows.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function stylegan_start() {
                      document.getElementById('stylegan_knows').style.opacity = "1";
                    }

                    function stylegan_stop() {
                      document.getElementById('stylegan_knows').style.opacity = "0";
                    }
                    stylegan_stop()
                  </script>
                </td>
                <td class="paper-details">
                  <a href="https://arxiv.org/abs/2306.00987" target="_blank">StyleGAN knows Normal, Depth, Albedo and more</a>
                  <br>
                  <b>Anand Bhattad</b>, <a href="https://www.danielbmckee.com/">Daniel McKee</a>,  <a href="https://dhoiem.cs.illinois.edu">Derek Hoiem</a>, <a href="http://luthuli.cs.uiuc.edu/~daf/">David Forsyth</a>
                  <br>
                  <em><strong>arXiv</strong></em>, 2023
                  <br>
                  <a href="https://arxiv.org/abs/2306.00987" target="_blank"><em>arXiv</em></a> 
                  <p></p>
                  <p> 
                  StyleGAN has easy accssible internal encoding of intrinsic images as originally defined by Barrow and Tenenbaum in their influential paper of 1978.</p>
                </td>
              </tr>
            <!-- StyLitGAN: Prompting StyleGAN to Produce New Illumination Conditions -->
              <tr onmouseout="stylit_stop()" onmouseover="stylit_start()">
                <td class="paper-img">
                  <div class="one">
                    <div class="two" id='stylit_image'>
                      <img src='stylitgan/splash/1_relight1.png' width="160">
                    </div>
                    <img src='stylitgan/splash/1_orig.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function stylit_start() {
                      document.getElementById('stylit_image').style.opacity = "1";
                    }

                    function stylit_stop() {
                      document.getElementById('stylit_image').style.opacity = "0";
                    }
                    stylit_stop()
                  </script>
                </td>
                <td class="paper-details">
                  <a href="https://arxiv.org/abs/2205.10351" target="_blank">
                    <div class="papertitle">StyLitGAN: Prompting StyleGAN to Produce New Illumination Conditions
                    </div>
                  </a>
                  <strong> Anand Bhattad</strong>,
                  <a href="http://luthuli.cs.uiuc.edu/~daf/" target="_blank">David A. Forsyth</a>
                  <br>
                  <em><strong>arXiv</strong></em>, 2023
                  <br>
                  <a href="https://arxiv.org/abs/2205.10351" target="_blank"><em>arXiv</em></a> <a href="https://anandbhattad.github.io/stylitgan/" target="_blank"><em>Project</em></a>
                  <p></p>
                  <p> By imposing known physical facts about images, we can prompt StyleGAN to generate relighted or resurfaced images without using labeled data. </p>
                </td>
              </tr>
              <tr onmouseout="makeitso_stop()" onmouseover="makeitso_start()">
                <td class="paper-img">
                  <div class="one">
                    <div class="two" id='makeitso_image'>
                      <img src='makeitso/static/images/bedroom/dreamy_stays/dreamy_stays_recolor.png' width="160">
                    </div>
                    <img src='makeitso/static/images/bedroom/dreamy_stays/orig.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function makeitso_start() {
                      document.getElementById('makeitso_image').style.opacity = "1";
                    }

                    function makeitso_stop() {
                      document.getElementById('makeitso_image').style.opacity = "0";
                    }
                    makeitso_stop()
                  </script>
                </td>
                <td class="paper-details">
                  <a href="https://arxiv.org/abs/2304.14403" target="_blank">
                    <div class="papertitle">Make It So: Steering StyleGAN for Any Image Inversion and Editing
                    </div>
                  </a>
                  <strong> Anand Bhattad</strong>, 
                    <a href="https://virajshah.com">Viraj Shah</a>
                    <a href="https://dhoiem.cs.illinois.edu">Derek Hoiem</a>,
                  <a href="http://luthuli.cs.uiuc.edu/~daf/" target="_blank">David A. Forsyth</a>
                  <br>
                  <em><strong>arXiv</strong></em>, 2023
                  <br>
                  <a href="https://arxiv.org/abs/2304.14403" target="_blank"><em>arXiv</em></a>, <a href="https://anandbhattad.github.io/makeitso/" target="_blank"><em>Project</em></a>

                  <p></p>
                  <p> A novel near-perfect GAN Inversion method that preserves editing capabilities, even for out-of-domain images </p>
                </td>
              </tr>
              <tr onmouseout="illuminator_stop()" onmouseover="illuminator_start()">
                <td class="paper-img">
                  <div class="one">
                    <div class="two" id='illuminator_image'>
                      <img src='images/illuminator_ours.png' width="160">
                    </div>
                    <img src='images/illuminator_cutpaste_marking.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function illuminator_start() {
                      document.getElementById('illuminator_image').style.opacity = "1";
                    }

                    function illuminator_stop() {
                      document.getElementById('illuminator_image').style.opacity = "0";
                    }
                    illuminator_stop()
                  </script>
                </td>
                <td class="paper-details">
                  <a href="">
                     <div class="papertitle">Illuminator: Learning to Correct Lighting without Ground Truth Data </div>

                  </a>
                  
                  <strong> Anand Bhattad</strong>,
                  <a href="https://brianc5.github.io/" target="_blank">Brian Chen</a>,
                  <a href="https://shenlong.web.illinois.edu/" target="_blank">Shenlong Wang</a>,
                  <a href="http://luthuli.cs.uiuc.edu/~daf/" target="_blank">David A. Forsyth</a>
                  <br>
                  <!-- <em><strong>arXiv</strong></em>, 2023 -->
                  <!-- <br>
                  <a href=""><em>Coming Soon</em></a> -->
                  <p></p>
                  <p> First self-supervised, image-based object relighting method trained without labeled paired data,
                    CGI data, geometry, or environment maps.</p>
                </td>
              </tr>
              <tr onmouseout="sirfyn_stop()" onmouseover="sirfyn_start()">
                <td class="paper-img">
                  <div class="one">
                    <div class="two" id='sirfyn_image'>
                      <img src='images/relighter_2170.gif' width="160">
                    </div>
                    <img src='images/relighter_2170.jpg' width="160">
                  </div>
                  <script type="text/javascript">
                    function sirfyn_start() {
                      document.getElementById('sirfyn_image').style.opacity = "1";
                    }

                    function sirfyn_stop() {
                      document.getElementById('sirfyn_image').style.opacity = "0";
                    }
                    sirfyn_stop()
                  </script>
                </td>
                <td class="paper-details">
                  <a href="https://arxiv.org/abs/2112.04497" target="_blank">
                     <div class="papertitle">SIRfyN: Single Image Relighting from your Neighbors </div>

                  </a>
                  <a href="http://luthuli.cs.uiuc.edu/~daf/" target="_blank">David A. Forsyth</a>,
                  <strong> Anand Bhattad</strong>,
                  <a href="https://github.com/pranav-asthana" target="_blank">Pranav Asthana</a>,
                  <a href="https://www.linkedin.com/in/yuanyi-zhong/" target="_blank">Yuani Zhong</a>,
                  <a href="https://yxw.web.illinois.edu/" target="_blank">Yuxiong Wang</a>
                  <br>
                  <em><strong>arXiv</strong></em>, 2022
                  <br>
                  <a href="https://arxiv.org/abs/2112.04497" target="_blank"><em>Technical Report</em></a>
                  <p></p>
                  <p> First scene relighting method that requires no
                    labeled or paired image data.</p>
                </td>
              </tr>
              <tr onmouseout="reshade_stop()" onmouseover="reshade_start()">
                <td class="paper-img">
                  <div class="one">
                    <div class="two" id='reshade_image'>
                      <img src='images/reshaded.png' width="160">
                    </div>
                    <img src='images/cutpaste.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function reshade_start() {
                      document.getElementById('reshade_image').style.opacity = "1";
                    }

                    function reshade_stop() {
                      document.getElementById('reshade_image').style.opacity = "0";
                    }
                    reshade_stop()
                  </script>
                </td>
                <td class="paper-details">
                  <a href="https://anandbhattad.github.io/projects/dipr.pdf" target="_blank">
                     <div class="papertitle">Cut-and-Paste Object Insertion by Enabling Deep Image Prior for Reshading </div>

                  </a>
                  <strong> Anand Bhattad</strong>,
                  <a href="http://luthuli.cs.uiuc.edu/~daf/" target="_blank">David A. Forsyth</a>
                  <br>
                  <em>Internation Conference on 3D Vision (3DV)</em>, 2022
                  <br>
                  <a href="https://anandbhattad.github.io/projects/reshading/index.html" target="_blank">
                    <em>Project</em>
                  </a>
                  <p></p>
                  <p>Convincing cut-and-paste reshading with consistent image decomposition inferences.</p>
                </td>
              </tr>
              <tr onmouseout="diver_stop()" onmouseover="diver_start()">
                <td class="paper-img">
                  <div class="one">
                    <div class="two" id='diver_image'>
                      <img src='images/nerf_fail_ours.png' width="160">
                    </div>
                    <img src='images/nerf_fail.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function diver_start() {
                      document.getElementById('diver_image').style.opacity = "1";
                    }

                    function diver_stop() {
                      document.getElementById('diver_image').style.opacity = "0";
                    }
                    diver_stop()
                  </script>
                </td>
                <td class="paper-details">
                  <a href="https://arxiv.org/abs/2111.10427" target="_blank">
                     <div class="papertitle">DIVeR: Real-time and Accurate Neural Radiance Fields with Deterministic Integration for
                      Volume Rendering </div>

                  </a>
                  <a href="https://lwwu2.github.io/" target="_blank">Liwen Wu</a>,
                  <a href="https://jyl.kr/" target="_blank">Jae Yong Lee</a>,
                  <strong> Anand Bhattad</strong>,
                  <a href="https://yxw.web.illinois.edu/" target="_blank">Yuxiong Wang</a>,
                  <a href="http://luthuli.cs.uiuc.edu/~daf/" target="_blank">David A. Forsyth</a>
                  <br>
                  <em><strong>Computer Vision and Pattern Recognition (CVPR)</strong></em>, 2022 <strong
                    style="color:#911a1a;">(Best Paper Finalist)</strong>
                  <br>
                  <a href="https://lwwu2.github.io/diver/" target="_blank"><em>Project</em></a> /
                  <a href="https://github.com/lwwu2/diver" target="_blank">
                    <em>Training Code</em></a> /
                  <a href="https://github.com/lwwu2/diver-rt" target="_blank">
                    <em>Real-time Code</em></a>
                  <p></p>
                  <p> Improving Real-Time NeRF with Deterministic Integration.</p>
                </td>
              </tr>
              <tr onmouseout="tex3Dinference_stop()" onmouseover="tex3Dinference_start()">
                <td class="paper-img">
                  <div class="one">
                    <div class="two" id='tex3Dinference_image'>
                      <img src='images/car3D.gif' width="160">
                    </div>
                    <img src='images/car_single_view.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function tex3Dinference_start() {
                      document.getElementById('tex3Dinference_image').style.opacity = "1";
                    }

                    function tex3Dinference_stop() {
                      document.getElementById('tex3Dinference_image').style.opacity = "0";
                    }
                    tex3Dinference_stop()
                  </script>
                </td>
                <td class="paper-details">
                  <a href="https://arxiv.org/abs/2106.06533" target="_blank">
                     <div class="papertitle">View Generalization for Single Image Textured 3D Models </div>
                  </a>
                  <strong> Anand Bhattad</strong>,
                  <a href="https://scholar.google.com/citations?user=pvu770UAAAAJ&hl=en" target="_blank">Aysegul Dundar</a>,
                  <a href="https://liuguilin1225.github.io/" target="_blank">Guilin Liu</a>,
                  <a href="https://scholar.google.com/citations?user=Wel9l1wAAAAJ&hl=en" target="_blank">Andrew Tao</a>,
                  <a href="https://ctnzr.io/" target="_blank">Bryan Catanzaro</a>
                  <br>
                  <em><strong>Computer Vision and Pattern Recognition (CVPR)</strong></em>, 2021
                  <br>
                  <a href="https://nv-adlr.github.io/view-generalization/" target="_blank"><em>Project</em></a>
                  <p></p>
                  <p> Consistent textured 3D inferences from a single 2D image.</p>
                </td>
              </tr>
              <tr onmouseout="adv_stop()" onmouseover="adv_start()">
                <td class="paper-img">
                  <div class="one">
                    <div class="two" id='adv_image'>
                      <img src='images/adv_golfcart.png' width="160" height="160">
                    </div>
                    <img src='images/real.png' width="160" height="160">
                  </div>
                  <script type="text/javascript">
                    function adv_start() {
                      document.getElementById('adv_image').style.opacity = "1";
                    }
                    function adv_stop() {
                      document.getElementById('adv_image').style.opacity = "0";
                    }
                    adv_stop()
                  </script>
                </td>
                <td class="paper-details">
                  <a href="https://arxiv.org/abs/1904.06347" target="_blank">
                     <div class="papertitle">Unrestricted Adversarial Examples via Semantic Manipulation </div>
                  </a>
                  <strong> Anand Bhattad*</strong>,
                  <a href="https://mchong6.github.io/" target="_blank">Min Jin Chong*</a>,
                  <a href="https://kaizhaoliang.github.io/Portfolio/" target="_blank">Kaizhao Liang</a>,
                  <a href="https://aisecure.github.io/" target="_blank">Bo Li</a>,
                  <a href="http://luthuli.cs.uiuc.edu/~daf/" target="_blank">David A. Forsyth</a>
                  <br>
                  <em><strong>International Conference on Learning Representations (ICLR)</strong></em>, 2020
                  <br>
                  <em>CVPR-W on <a href="https://amlcvpr2019.github.io/">Adversarial ML in Real-World Computer Vision
                      Systems</a></em>, 2019
                  <br>
                  <p></p>
                  <p>Generating realistic adversarial examples by image re-colorization and texture transfer.</p>
                </td>
              </tr>
              <tr onmouseout="style_stop()" onmouseover="style_start()">
                <td class="paper-img">
                  <div class="one">
                    <img src='images/EvalStyleTransfer.png' width="160" height="160">
                  </div>
                </td>
                <td class="paper-details">
                  <a href="https://arxiv.org/abs/1910.09447" target="_blank">
                     <div class="papertitle">Improved Style Transfer with Calibrated Metrics </div>
                  </a>
                  <a href="https://dblp.org/pers/hd/y/Yeh:Mao=Chuang" target="_blank">Mao Chuang Yeh*</a>,
                  <a href="">Shuai Tang*</a>,
                  <strong> Anand Bhattad</strong>,
                  <a href="http://czou4.web.engr.illinois.edu/" target="_blank">Chuhang Zou</a>,
                  <a href="http://luthuli.cs.uiuc.edu/~daf/" target="_blank">David A. Forsyth</a>
                  <br>
                  <em><strong>Winter Conference on Applications of Computer Vision (WACV)</strong></em>, 2020
                  <br>
                  <p></p>
                  <p> A novel quantitative evaluation procedure for style transfer methods.</p>
                </td>
              </tr>

              <tr onmouseout="anom_stop()" onmouseover="anom_start()">
                <td class="paper-img">
                  <div class="one">
                    <div class="two" id='anom_image'>
                      <img src='images/anomaly.png' width="160">
                    </div>
                    <img src='images/typical.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function anom_start() {
                      document.getElementById('anom_image').style.opacity = "1";
                    }

                    function anom_stop() {
                      document.getElementById('anom_image').style.opacity = "0";
                    }
                    anom_stop()
                  </script>
                </td>
                <td class="paper-details">
                  <a href="https://arxiv.org/abs/1802.05798" target="_blank">
                     <div class="papertitle">Detecting Anomalous Faces with "No Peeking'' Autoencoders </div>
                  </a>
                  <br>
                  <strong> Anand Bhattad</strong>,
                  <a href="https://scholar.google.com/citations?user=107hH58AAAAJ&hl=en" target="_blank">Jason Rock</a>,
                  <a href="http://luthuli.cs.uiuc.edu/~daf/" target="_blank">David A. Forsyth</a>
                  <br>
                  <em><strong>CVPR Workshop</strong> on <a href="https://vbsd2018.github.io/" target="_blank">Vision with Biased or
                      Scarce Data</a></em>, 2018
                  <br>
                  <p> A simple unsupervised method for detecting anomalous faces by carefully constructing features from
                    "No Peeking" or inpainting autoencoders. </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <p>
                <heading id="teaching"> Teaching</heading>
              </p>

        </td>
        <td width="75%" valign="center">
          Graduate Teaching Assistant, <a href="http://luthuli.cs.uiuc.edu/~daf/courses/AML-18-Fall/aml-home.html" target="_blank">CS498
            Applied Machine Learning</a>, Fall 2018
          <br>
          <br>
          Graduate Teaching Assistant, <a href="">CS225 Data Structures</a>, Spring 2017
          <br>
          <br>
          Graduate Teaching Assistant, <a href="">CS101 Intro Computer Science</a>, Spring 2016 <font color="#911a1a">
            <strong>(Ranked as Outstanding TA)</strong>
          </font> & Fall 2017
        </td>
      </tr>
    </tbody>
  </table>
  <p></p>
  <br>
  <div style="text-align: right;"> Template credit: <a href="https://jonbarron.info/" target="_blank">Jon Barron</a>.</div>

</html>